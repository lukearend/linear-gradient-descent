{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d58d48-6f4b-4601-8b0b-d2a3ba31b716",
   "metadata": {},
   "source": [
    "# The dynamics of learning in deep linear networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "75c2021f-0641-483f-b977-565ab08f76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d9c99-238f-417a-8c0d-5ee1ad2c55ab",
   "metadata": {},
   "source": [
    "Here we implement the deep linear network trained by gradient descent and test the claims made in \"Exact solution to the nonlinear dynamics of learning in deep linear neural networks\" by Andrew Saxe, James McClelland, and Surya Ganguli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9823cef-8899-4b44-a96b-118048adb5f7",
   "metadata": {},
   "source": [
    "# Part 1: setting the stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698b4c-0a92-4bb2-8984-6271875a42cf",
   "metadata": {},
   "source": [
    "### Target function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc34b37-3fd2-4e3f-ad7e-bc96a53cc4b6",
   "metadata": {},
   "source": [
    "We want a linear network to approximate a **target function** $f$ which maps $N$-vectors to $M$-vectors. \n",
    "\n",
    "To program the network we need a **dataset** of examples of the input-output mapping $\\mathcal D = x^\\mu, y^\\mu$ for $\\mu = 1, 2 \\dots P$.\n",
    "\n",
    "We take $X$ as the first 100 (whitened) columns of the first 1000 MNIST examples, and $Y$ as a one-hot coded category label.\n",
    "\n",
    "So $N = 100$, $M = 10$, and $P = 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6326a154-7ff7-4b3e-98a7-a8d5ffefcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "N = 100\n",
    "P = 50000\n",
    "X, Y = pd.read_pickle('mnist.pkl')\n",
    "X = X[:P]\n",
    "Y = Y[:P]\n",
    "X = X - X.mean(axis=0)\n",
    "U, S, VT = np.linalg.svd(X.T @ X)\n",
    "X = X @ VT[:N].T\n",
    "X = X / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158f5cd-03d0-4d03-8dcf-7b21a82c272c",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64828b0d-37e9-438f-ac6d-b96b34ba50fb",
   "metadata": {},
   "source": [
    "The architecture of an $L$-layer linear network is described by the layer sizes $(N_0, N_1 \\dots N_L)$.\n",
    "\n",
    "In this convention, layer $0$ assumes the value of the input vector and layer $L$ assumes the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "30025c9a-f215-45bf-83c3-b402f6b3e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer linear network with 100 hidden units\n",
    "Nl = [N, 32, M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a11962-cdf8-4c63-a7f6-72c855352ed2",
   "metadata": {},
   "source": [
    "The network computes a **programmable mapping** $g$ parameterized by the matrices $(W_1, W_2 \\dots W_L)$.\n",
    "\n",
    "$W^l$ is a $N_l \\times N_{l - 1}$ matrix that maps vectors from layer $l - 1$ to $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d164b2f8-9872-452c-bd05-495bf3243984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights as norm-preserving random projections\n",
    "Wl = []\n",
    "for Nin, Nout in zip(Nl[:-1], Nl[1:]):\n",
    "    W = np.random.randn(Nout, Nin)\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True)\n",
    "    Wl.append(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5986cff-b232-4248-ad7d-56f6881dc25c",
   "metadata": {},
   "source": [
    "The network computes the function\n",
    "\n",
    "$$g(X) = X {W_1}^T {W_2}^T \\dots {W_L}^T$$\n",
    "\n",
    "when applied to matrix $X$ with input vectors $x$ on its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9be377b4-6de4-4466-a789-c3f701628087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass computes internal activation and output\n",
    "def forward(X, Wl):\n",
    "    Xl = [X]\n",
    "    for W in Wl:\n",
    "        X = X @ W.T\n",
    "        Xl.append(X)\n",
    "    gX = Xl[-1]\n",
    "    return gX, Xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "978222d6-833d-4224-8a13-c461b9df60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map X to g(X)\n",
    "gX = forward(X, Wl)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e76c8a-46ae-4af4-b355-b603ef0f516f",
   "metadata": {},
   "source": [
    "We want to optimize $g$ for approximating the target mapping $f$ over the dataset $\\mathcal D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef26d1b-0746-4ebf-a411-3f56202c9435",
   "metadata": {},
   "source": [
    "### Optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25a6cc-a888-4f4a-b4a9-6b870a0fad8e",
   "metadata": {},
   "source": [
    "To do this we define an **objective function $\\mathcal L$** that outputs a quantity called the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb96a2-1813-400f-8724-4fe460b49a01",
   "metadata": {},
   "source": [
    "The goal of gradient descent is to find $(W^1, W^2 \\dots W^L)$ that minimize the loss given $\\mathcal D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30831954-d2de-4d23-bfad-4828cdc25302",
   "metadata": {},
   "source": [
    "As a loss function we minimize squared error, where error $E = f(X) - g(X)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0fff7d4d-1b24-42c5-b7ca-02e4134e709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss measures model error over the training data\n",
    "def loss(fX, gX):\n",
    "    return np.mean((fX - gX) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cb5f4762-677c-43a5-8f70-753f608662a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1043516264560895"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the loss\n",
    "loss(Y, gX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5dc9ea-4316-40bf-9b1b-b99b226d1b6e",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2d9c99-18ce-46b6-adeb-52f9e7237dd8",
   "metadata": {},
   "source": [
    "Observe that $\\frac{dL}{dE} = 2 \\cdot E$ and $\\frac{dE}{dg} = -1$.\n",
    "\n",
    "Then dropping a constant $2$ and denoting activation at the output layer $X_L$,\n",
    "\n",
    "$$\\frac{d\\mathcal L}{dX_L} = g(X) - f(X).$$\n",
    "\n",
    "Since layer $l$ computes $X_{l-1} {W_l}^T$, the gradient of its output with respect to its input is $W^T$.\n",
    "\n",
    "Then denoting activations at layer $l$ as $X_l$,\n",
    "\n",
    "$$\\frac{dX_l}{dX_{l - 1}} = {W_l}^T$$\n",
    "\n",
    "Likewise, the gradient of its weight matrix ${W_l}^T$ with respect its output is $X_{l-1}$, the layer's input:\n",
    "\n",
    "$$\\frac{dX_l}{{dW_l}^T} = {X_{l - 1}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74322abe-91c8-487d-9ffd-da64253bc460",
   "metadata": {},
   "source": [
    "Using these equations we can propagate gradients backward from the loss to any part of the network.\n",
    "\n",
    "We stop at each layer $l$ and compute the gradients of $\\mathcal L$ with respect $W_l$ using that layer's activation:\n",
    "\n",
    "$$\\frac{d\\mathcal L}{dW_l^T} = \\frac{d\\mathcal L}{dX_L} \\frac{dX_L}{dX_{L-1}} \\dots \\frac{dX_{l+1}}{dX_l} \\frac{dX_l}{dW_l^T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2d5ef524-6499-4028-b909-14e0db84b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass computes gradients of loss with respect to weights\n",
    "def backward(fX, Xl):\n",
    "    E = fX - Xl.pop()\n",
    "    dLdg = -2 * E\n",
    "    dLdWl = []\n",
    "    for W in Wl[::-1]:\n",
    "        X = Xl.pop()\n",
    "        dLdW = np.sum(dLdg[:, :, np.newaxis] * X[:, np.newaxis, :], axis=0)\n",
    "        dLdg = np.sum(dLdg[:, :, np.newaxis] * W[np.newaxis, :, :], axis=1)\n",
    "        dLdWl.append(dLdW)\n",
    "    return dLdWl[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "98f6960b-1f3d-468b-a607-afe0ca86f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the backward pass\n",
    "Xl = forward(X, Wl)[-1]\n",
    "dLdWl = backward(Y, Xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7794d-9793-444b-9535-aa490305ca5e",
   "metadata": {},
   "source": [
    "### Stepwise update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d78ae-0029-4c9d-ba4b-4927cda5b653",
   "metadata": {},
   "source": [
    "Gradient descent iteratively updates $(W^1, W^2 \\dots W^L)$ in a manner that reduces $\\mathcal L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c836d-48fa-40b6-82c3-e0459b1582fc",
   "metadata": {},
   "source": [
    "This is achieved by applying the **update rule**\n",
    "\n",
    "$$d W_l = -\\lambda \\cdot \\frac{d\\mathcal L}{dW_l}$$\n",
    "\n",
    "where $\\lambda$ is a small learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d79fe760-b666-40e6-8639-6cff963950f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(Wl, dLdWl, lr):\n",
    "    for W, dLdW in zip(Wl, dLdWl):\n",
    "        W -= lr * dLdW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5750e-8385-4e4f-866b-b4a4f848883d",
   "metadata": {},
   "source": [
    "Each step of gradient descent updates each weight matrix slightly in a direction opposite to its gradient with respect to the loss.\n",
    "\n",
    "This reduces $g$'s actual error at approximating $f$ on $X$ and expected error at approximating $f$ on an input distribution like $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "638cec42-cd99-474d-bf5d-e9e6ef0162bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights as norm-preserving random projections\n",
    "Wl = []\n",
    "for Nin, Nout in zip(Nl[:-1], Nl[1:]):\n",
    "    W = np.random.randn(Nout, Nin)\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True)\n",
    "    Wl.append(W)\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc16580-638a-4b9e-ba70-14eab2e390a0",
   "metadata": {},
   "source": [
    "### A step of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea95c5f-1820-48c7-a859-1cdb5043f322",
   "metadata": {},
   "source": [
    "We may compute the loss prior to any training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c9ff739f-7712-41f9-8514-6dc168473999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(t=0): 0.936852\n"
     ]
    }
   ],
   "source": [
    "gX = forward(X, Wl)[0]\n",
    "print(f\"loss(t={t}): {loss(Y, gX):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806d8e1-0b89-490c-961f-bef8ce077422",
   "metadata": {},
   "source": [
    "One step of gradient descent looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "05007f40-0555-4771-b214-25dca8401fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdWl = backward(Y, forward(X, Wl)[-1])\n",
    "update(Wl, dLdWl, lr=1e-6)\n",
    "t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e4a7f-ac44-431c-a6fd-42e1d2ac941b",
   "metadata": {},
   "source": [
    "Notice the loss has descreased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "86846d12-63a6-40d8-bc5a-f8e461b826a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(t=1): 0.594968\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss(t={t}): {loss(Y, forward(X, Wl)[0]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a22082-a67c-483e-bd3f-7e4e6363472c",
   "metadata": {},
   "source": [
    "### Solving a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b86ca2a4-5851-4eaa-9b0e-3f8d3a863317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(fX, gX):\n",
    "    return np.mean(np.argmax(fX, axis=1) == np.argmax(gX, axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ee28e9-f0b4-40d5-8f57-afdd6e4d2d94",
   "metadata": {},
   "source": [
    "Let's see if the network can learn to classify MNIST digits using the $\\arg \\max$ decision rule.\n",
    "\n",
    "Before any training, its accuracy is around 10% (chance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "da3f5363-00e3-40ec-8f0e-f79b5278c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(t=0): 0.11006\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy(t=0): {accuracy(Y, gX)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298194c-0fca-4889-a09c-142830e2d0ab",
   "metadata": {},
   "source": [
    "After training for 1 step, the accuracy slightly improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ca4b50bc-4f9a-4993-98ea-a858c2ded2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(t=1): 0.12794\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy(t=1): {accuracy(Y, forward(X, Wl)[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ccdc3-995d-4cdc-a1ce-4960551a5862",
   "metadata": {},
   "source": [
    "Now, we train the network for 9 more steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "5e97064b-005f-47ab-9a9b-b9dca847d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec4bcfb27ac484293a6d0d20e286e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(t=10): 0.38976\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(9)):\n",
    "    dLdWl = backward(Y, forward(X, Wl)[-1])\n",
    "    update(Wl, dLdWl, lr=1e-6)\n",
    "    t += 1\n",
    "print(f\"accuracy(t={t}): {accuracy(Y, forward(X, Wl)[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4318021-38d3-439f-8345-061e80672df5",
   "metadata": {},
   "source": [
    "And then 90 more steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4327a2d8-fd72-4969-a388-f1d444743d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccb3b52a8994c68b71ffd2dd425e7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(t=100): 0.82956\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(90)):\n",
    "    dLdWl = backward(Y, forward(X, Wl)[-1])\n",
    "    update(Wl, dLdWl, lr=1e-6)\n",
    "    t += 1\n",
    "print(f\"accuracy(t={t}): {accuracy(Y, forward(X, Wl)[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a972679-f1ca-472d-908f-cc95f8d052d5",
   "metadata": {},
   "source": [
    "After 100 gradient descent steps the accuracy above 80%. Therefore **the network learns** to solve a task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2abba-ddfb-41eb-a2c1-0e71b0915cba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0121b-dc15-4e35-8880-b29721a19937",
   "metadata": {},
   "source": [
    "# Part 2: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c32bb9-1f54-4d17-901a-2b4e40722c71",
   "metadata": {},
   "source": [
    "The authors write that due to the linearity of the network, it can be trained by pre-computing $\\text{cov}[f(X), g(X)]$ and running the forward-backward pass on it instead of the full dataset of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7b471ed1-c033-4c3c-946d-f360fb844361",
   "metadata": {},
   "outputs": [],
   "source": [
    "CXX = X.T @ X / len(X)\n",
    "CYX = Y.T @ X / len(Y)\n",
    "CYY = Y.T @ Y / len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f036e41-6795-4aee-a155-ed9f8c83b937",
   "metadata": {},
   "source": [
    "Using these we can write an equivalent **covariance update rule**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "8205d011-2232-445f-bc6e-058ce639e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(Ws, CXX, CYX, lr):\n",
    "    dLdg = compose(Ws) @ CXX - CYX\n",
    "    for i, W in enumerate(Ws):\n",
    "        if i == 0:\n",
    "            dLdW = compose(Ws[1:]).T @ dLdg\n",
    "        elif i == len(Ws) - 1:\n",
    "            dLdW = dLdg @ compose(Ws[:-1]).T\n",
    "        else:\n",
    "            dLdW = compose(Ws[i + 1:]).T @ dLdg @ compose(Ws[:i]).T\n",
    "        W -= lr * dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e63251-f648-420f-a64f-cefaaab24d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
