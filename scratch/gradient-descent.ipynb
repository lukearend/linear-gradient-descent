{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af20b7b7-1aa9-43db-a114-4ecfe477b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01763ef-7284-47f3-a57b-8cbb1ec64bbd",
   "metadata": {},
   "source": [
    "# Programming a linear network to classify MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d269d56-8f41-4d14-8217-602239e34e42",
   "metadata": {},
   "source": [
    "### The target function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87bb8b-5f5b-4bb0-930f-0eb8e0bc0237",
   "metadata": {},
   "source": [
    "We start with a vector-valued **target function** $f$ we wish to approximate:\n",
    "$$\n",
    "f: \\vec x \\mapsto \\vec y \\quad \\vec x \\in \\mathbb{R}^N, \\vec y \\in \\mathbb{R}^M\n",
    "$$\n",
    "\n",
    "Here we specify an $f$ by sampling a random \"teacher\" $W^* \\in \\mathbb R^{M \\times N}$. $W^*$ sends a $N$-vector $\\vec x$ to an $M$-vector $W \\vec x$ by left multiplication, or a $P\\times N$ matrix $X$ with $N$-vectors along its rows to a $P\\times M$ matrix $X @ W.T$ with $M$-vectors along its rows. This target function is a linear mapping, meaning that in principle a linear neural network can fit it perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44edb8b-0193-4f7c-a6e1-83dbd2aaed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "N = 100\n",
    "Wstar = np.random.randn(M, N)\n",
    "f = lambda X: X @ Wstar.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c4aa0-387f-4469-9232-205b031b65d0",
   "metadata": {},
   "source": [
    "This gives rise to a **dataset** $\\mathcal D$ of examples from $f$:\n",
    "$$\n",
    "\\mathcal D = \\{(\\vec x^\\mu, \\vec y^\\mu)\\} \\quad \\mu = 1 \\dots P\n",
    "$$\n",
    "We sample $X$ from a standard normal and apply $f$ to make $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5050e3-2bf0-4371-aeb6-c2585284ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1000, 100), Y: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "P = 1000\n",
    "X = Xall[:1000]\n",
    "Y = f(X)\n",
    "\n",
    "print(f'X: {X.shape}, Y: {Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7592e-c7f2-47fa-98a6-f6008ccab5ba",
   "metadata": {},
   "source": [
    "### The network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110ddbf-6d60-4cf6-815c-f5840087332f",
   "metadata": {},
   "source": [
    "Gradient descent operates on a **tunable map** $g$ which we will use to approximate $f$:\n",
    "$$\n",
    "g: \\vec x \\mapsto W \\vec x \\quad W \\in \\mathbb{R}^{M \\times N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49228af0-f884-45e6-a48a-0c5c41d28cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, N1, N2, N3):\n",
    "        self.W21 = np.random.randn(N2, N1)\n",
    "        self.W32 = np.random.randn(N3, N2)\n",
    "        self.g1 = None\n",
    "        self.g2 = None\n",
    "        self.g3 = None\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.g1 = X\n",
    "        self.g2 = self.g1 @ self.W21.T\n",
    "        self.g3 = self.g2 @ self.W32.T\n",
    "        return self.g3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4d728-ea9c-4074-a5a0-3ecfc3d8f3bd",
   "metadata": {},
   "source": [
    "Here we use a two-layer linear network\n",
    "\n",
    "$g(\\vec x) = W^{32} W^{21} \\vec x$ parameterized by $W^{21} \\in \\mathbb{R}^{N_2 \\times N_1}$, $W^{32} \\in \\mathbb{R}^{N_3 \\times N_2}$.\n",
    "\n",
    "The network has $N_1$ input units, $N_2$ hidden units and $N_3$ output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9848cf2a-f3c1-437c-87f6-8041a46927e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W21: (20, 100), W32: (10, 20), g(X): (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "g = Network(N, 20, M)\n",
    "gX = g(X)\n",
    "\n",
    "print(f'W21: {g.W21.shape}, W32: {g.W32.shape}, g(X): {gX.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccefb40-f02d-4a7b-aaef-65872ca0e6ce",
   "metadata": {},
   "source": [
    "### The objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4a29c-6b3a-42b4-a23c-11204c20d7b1",
   "metadata": {},
   "source": [
    "Gradient descent employs an **objective function** $\\mathcal L$ which we minimize with respect to $g$ over $\\mathcal D$:\n",
    "$$\n",
    "\\mathcal L_\\mathcal D(g) = \\sum_{\\mu = 1}^P \\lVert g(x) - f(x) \\rVert_2^2\n",
    "$$\n",
    "We use squared error across output units, averaged over batches of size $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "720b5007-b1d8-4a92-9c47-989aeea1852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 22516.691148140602, error per unit: 15.005562684598203\n"
     ]
    }
   ],
   "source": [
    "E = lambda fX, gX: fX - gX\n",
    "L = lambda fX, gX: np.sum(E(fX, gX) ** 2) / P\n",
    "\n",
    "loss = L(Y, gX)\n",
    "print(f'loss: {loss}, error per unit: {np.sqrt(loss) / M}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca5958-34a3-4ca6-b240-282a530ab046",
   "metadata": {},
   "source": [
    "Minimizing $\\mathcal L$ involves repeatedly computing its **gradients** with respect to model parameters $W$:\n",
    "$$\n",
    "\\nabla \\mathcal L_\\mathcal D(W) = \\begin{bmatrix}\n",
    "    \\frac{\\partial \\mathcal L}{\\partial W_{11}} & \\cdots & \\frac{\\partial \\mathcal L}{\\partial W_{M1}} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\mathcal L}{\\partial W_{1N}} & \\cdots & \\frac{\\partial \\mathcal L}{\\partial W_{MN}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58479103-34d3-4421-b395-82e68904db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad W21 shape: (20, 100), rms: 0.042313856308547684\n",
      "grad W32 shape: (10, 20), rms: 0.22997267355484222\n"
     ]
    }
   ],
   "source": [
    "dgdW = lambda X: X[:, np.newaxis, :] # (X rows, W output, W input)\n",
    "dgdX = lambda W: W[np.newaxis, :, :] # (X rows, W output, X cols)\n",
    "dLdW = lambda dLdg, X: np.mean(dLdg[:, :, np.newaxis] * dgdW(X), axis=0) # (W output, W input)\n",
    "dLdX = lambda dLdg, W: np.sum(dLdg[:, :, np.newaxis] * dgdX(W), axis=1) # (X rows, X cols)\n",
    "\n",
    "def grads(fX, g):\n",
    "    dLdg3 = -2 * E(fX, g.g3) / P # gradient from loss to last layer\n",
    "    dLdW32 = dLdW(dLdg3, g.g2)   # gradient from loss to W32\n",
    "    dLdg2 = dLdX(dLdg3, g.W32)   # gradient from loss to hidden layer\n",
    "    dLdW21 = dLdW(dLdg2, g.g1)   # gradient from loss to W21\n",
    "    return dLdW21, dLdW32\n",
    "\n",
    "dLdW21, dLdW32 = grads(Y, g)\n",
    "print(f'grad W21 shape: {dLdW21.shape}, rms: {np.sqrt(np.mean(dLdW21 ** 2))}')\n",
    "print(f'grad W32 shape: {dLdW32.shape}, rms: {np.sqrt(np.mean(dLdW32 ** 2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a838d-7e79-4d46-bb62-9fdd90f915af",
   "metadata": {},
   "source": [
    "### The procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18478739-e76e-4df0-88b0-bd41c7fc98b5",
   "metadata": {},
   "source": [
    "To train, we repeatedly apply the **update rule** for one step of optimization:\n",
    "$$\n",
    "W_{t + 1} = W_t - \\lambda \\cdot \\nabla \\mathcal L_\\mathcal D(W_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8075a16c-0081-4754-af6b-070f14543240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(g, Y, lr=1):\n",
    "    dLdW21, dLdW32 = grads(Y, g)\n",
    "    g.W21 -= lr * dLdW21\n",
    "    g.W32 -= lr * dLdW32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf67f8-467e-4eab-9d74-a49dcad3b0a0",
   "metadata": {},
   "source": [
    "The update rule perturbs the weights in a way that reduces the loss on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b79433a-780f-4bd1-878e-2ce2d9770723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before: 22516.691148140602, loss after: 11344.605325611119\n"
     ]
    }
   ],
   "source": [
    "loss1 = L(Y, g(X))\n",
    "update(g, Y)\n",
    "loss2 = L(Y, g(X))\n",
    "print(f'loss before: {loss1}, loss after: {loss2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074dccb-9327-4314-ab21-ccac05ec9b8b",
   "metadata": {},
   "source": [
    "A full training run is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02ac0356-2cd5-4296-b964-6d21973d6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17a6894e-e87f-42bc-bc3b-6d008e4cb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 11344.605326\n",
      "step 1000 loss: 1.009923 (3.7 sec)\n",
      "step 2000 loss: 0.764607 (7.5 sec)\n",
      "step 3000 loss: 0.593271 (11.0 sec)\n",
      "step 4000 loss: 0.475792 (14.5 sec)\n",
      "step 5000 loss: 0.393624 (18.2 sec)\n",
      "step 6000 loss: 0.335833 (21.6 sec)\n",
      "step 7000 loss: 0.293393 (25.1 sec)\n",
      "step 8000 loss: 0.260080 (28.7 sec)\n",
      "step 9000 loss: 0.232293 (32.2 sec)\n",
      "step 10000 loss: 0.208130 (35.7 sec)\n"
     ]
    }
   ],
   "source": [
    "nsteps = 10000\n",
    "\n",
    "print(f'initial loss: {L(Y, g(X)):.6f}')\n",
    "t0 = time.time()\n",
    "for i in range(1, nsteps + 1):\n",
    "    g(X)\n",
    "    update(g, Y, lr=10)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f'step {i + 1} loss: {L(Y, g(X)):.6f} ({time.time() - t0:.1f} sec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba541533-c34a-478d-9f67-fdaf42541bac",
   "metadata": {},
   "source": [
    "### The outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b1a70-9f45-4803-be40-fb9b7dba79c4",
   "metadata": {},
   "source": [
    "Given enough time, the network can perfectly fit the training data (it sends the loss to 0).\n",
    "\n",
    "We find that the network mapping $W^{32} W^{21}$ approximates $C^{31}$ after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c5153a8-3a6f-40c3-9844-8af717092e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C31|: 1.3984750918285602\n",
      "|W32|: 0.6781734253106437\n",
      "|W21|: 26.073818835182475\n",
      "|C31 - W32.W21|: 0.5357242405823567\n"
     ]
    }
   ],
   "source": [
    "print(f'|C31|: {np.sqrt(np.sum(C31 ** 2))}')\n",
    "print(f'|W32|: {np.sqrt(np.sum(g.W32 ** 2))}')\n",
    "print(f'|W21|: {np.sqrt(np.sum(g.W21 ** 2))}')\n",
    "print(f'|C31 - W32.W21|: {np.sqrt(np.sum((C31 - g.W32 @ g.W21) ** 2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1c417-fba6-403e-9e9c-a9bdffad9be5",
   "metadata": {},
   "source": [
    "### The setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc4fbc-4d46-41ce-8b09-6cdacaa5779c",
   "metadata": {},
   "source": [
    "Load whitened MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61fac95-6c96-4d7a-8288-7472d1f90dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mnist.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "Y = data['Y'] * 2 - 1\n",
    "X = (data['X'] - data['mu_whiten']) @ data['W_whiten'][:, :100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0e3da-3a95-4316-88b6-a0e07668bf55",
   "metadata": {},
   "source": [
    "Compute C31 = cov(Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed9f345-9ecd-4ef5-88e8-5a962bfaaac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C11 = X.T @ X / len(X)\n",
    "C31 = Y.T @ X / len(Y)\n",
    "C33 = Y.T @ Y / len(Y)\n",
    "Xall = X.copy()\n",
    "Yall = Y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c879cf-8d18-422c-975a-f106eaa77c38",
   "metadata": {},
   "source": [
    "Argument: the best a linear network can hope to do is make W32 W21 = C31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da7810-6c30-44b1-94d9-6b052c03a32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
