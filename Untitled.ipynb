{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfa97c1-e73c-4948-bd7d-d3f6d47aab8d",
   "metadata": {},
   "source": [
    "### Gradient descent in the linear network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b012721-adda-4119-aba4-7dcbdee214df",
   "metadata": {},
   "source": [
    "Load the MNIST dataset. It has 70000 rows and 784 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "0b2ab8a0-a4f7-4557-92d2-6febfe5beba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mnist.pkl', 'rb') as f:\n",
    "    mnist = pickle.load(f)\n",
    "X = mnist['X']\n",
    "Y = mnist['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d8b06-d74d-4b2e-8b27-43f58cf6b5a7",
   "metadata": {},
   "source": [
    "Whiten $X$ and take the first 100 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "c660594a-a814-4dc6-b341-96a4d63ebc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = X - X.mean(axis=0)\n",
    "U, S, W = np.linalg.svd(X.T @ X)\n",
    "X = X @ W[:100].T / np.sqrt(s[:100])\n",
    "X /= X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b5e48-16e2-43b2-96d2-6ed89810f6f1",
   "metadata": {},
   "source": [
    "Define the multi-layer linear network tuned by SGD with squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "74c3c216-430f-420b-8666-b97eb45f477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(layers, X, Y=None, lr=0):\n",
    "    g = [X]\n",
    "    for W in layers:\n",
    "        X = X @ W.T\n",
    "        g.append(X)\n",
    "    out = g.pop()\n",
    "    err = 0 * out if Y is None else Y - out\n",
    "    dLdg = -2 * err\n",
    "    for W in layers[::-1]:\n",
    "        X = g.pop()\n",
    "        dLdW = np.mean(dLdg[:, :, np.newaxis] * X[:, np.newaxis, :], axis=0)\n",
    "        dLdg = np.sum(dLdg[:, :, np.newaxis] * W[np.newaxis, :, :], axis=1)\n",
    "        W -= lr * dLdW\n",
    "    return out\n",
    "\n",
    "L = lambda fX, gX: np.mean(np.sum((fX - gX) ** 2, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c23b4-af4e-45dc-92ec-073017236983",
   "metadata": {},
   "source": [
    "Initialize a two-layer network with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "de7f5371-be84-422d-9a02-ba897a606975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "np.random.seed(0)\n",
    "N1, N2, N3 = (100, 10, 10)\n",
    "\n",
    "def random_W(M, N):\n",
    "    n = max(M, N)\n",
    "    Q, R = np.linalg.qr(np.random.randn(n, n))\n",
    "    return Q[:M, :N]\n",
    "\n",
    "W21 = random_W(N2, N1)\n",
    "W32 = random_W(N3, N2)\n",
    "g = functools.partial(mapping, [W21, W32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829195b-6304-4cae-bdf8-96e5e62ba7e5",
   "metadata": {},
   "source": [
    "Train the network until loss and accuracy plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "f9558ba7-cad4-4e94-ac4e-8ea6d98b93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loss: 10.659795190762066, accuracy: 0.13338571428571427\n",
      "loss after epoch 0: 1.3605439130761887, accuracy: 0.36551428571428574\n",
      "loss after epoch 1: 0.8704074676266697, accuracy: 0.5845285714285714\n",
      "loss after epoch 2: 0.7170727481955943, accuracy: 0.7106428571428571\n",
      "loss after epoch 3: 0.6387518666852239, accuracy: 0.7727571428571428\n",
      "loss after epoch 4: 0.5922787330623296, accuracy: 0.8034285714285714\n",
      "loss after epoch 5: 0.5640342529982761, accuracy: 0.8213857142857143\n",
      "loss after epoch 6: 0.5470163495276731, accuracy: 0.8305857142857143\n",
      "loss after epoch 7: 0.5368795570967739, accuracy: 0.8362285714285714\n",
      "loss after epoch 8: 0.5308914641079793, accuracy: 0.8401714285714286\n",
      "loss after epoch 9: 0.5273748472260247, accuracy: 0.8427714285714286\n"
     ]
    }
   ],
   "source": [
    "gX = g(X)\n",
    "acc = np.mean(np.argmax(gX, axis=1) == np.argmax(Y, axis=1))\n",
    "print(f'starting loss: {L(gX, Y)}, accuracy: {acc}')\n",
    "for epoch in range(10):\n",
    "    start = 0\n",
    "    for batch in range(70):\n",
    "        end = start + 100\n",
    "        step(X[start:end], Y[start:end], lr=0.01)\n",
    "        start = end\n",
    "    gX = g(X)\n",
    "    acc = np.mean(np.argmax(gX, axis=1) == np.argmax(Y, axis=1))\n",
    "    print(f'loss after epoch {epoch}: {L(gX, Y)}, accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522a046-5d00-44c5-ab03-d700fc61ab36",
   "metadata": {},
   "source": [
    "Observe that when network is solved, $W^{32} W^{21} X^T X \\to Y^T X$, which is to say $\\text{cov}[g(X), X] \\to \\text{cov}[Y, X]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "6f2ad704-477d-4fe2-a9b3-347109ca0b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004038315594681743"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYX = Y.T @ X / len(X)\n",
    "C11 = X.T @ X / len(X)\n",
    "C31 = W32 @ W21 @ C11\n",
    "np.sqrt(np.mean((CYX - C31) ** 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62225908-3959-4190-a76f-aa52e1a165e2",
   "metadata": {},
   "source": [
    "Recall that $X$ is whitened (i.e. $\\text{cov}[X] = I$). This means that $W^{32} W^{21} = \\text{cov}[Y, X]$ is a fixed point of the learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "fd465a38-becc-4ab1-b1b1-a69bdc1ba9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004038315594681735"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((CYX - W32 @ W21) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8147511-3f70-470b-8087-274802deb794",
   "metadata": {},
   "source": [
    "### Connectivity modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8be696-d7f8-482c-a94f-418b93c2258c",
   "metadata": {},
   "source": [
    "Consider the singular value decomposition $W^{32} W^{21} = C^{31} = U^{33} S^{31} {V^{11}}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "f4ed79d0-19e0-4298-9c91-f69f308c7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "U33, s, V11T = np.linalg.svd(C31)\n",
    "S31 = np.zeros((N3, N1))\n",
    "S31[:N3, :N3] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef42080-41d1-47c1-95e6-b3a657a0999a",
   "metadata": {},
   "source": [
    "Define $\\overline{W^{32}}$ and $\\overline{W^{21}}$ as a change of variables on the synaptic weight spaces $W^{32}$ and $W^{21}$:\n",
    "\n",
    "$\\overline{W^{32}} = {U^{33}}^T W^{32}$\n",
    "\n",
    "$\\overline{W^{21}}^T = W^{21} V^{11}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "59882142-d641-47c5-80d7-7ddcaac89382",
   "metadata": {},
   "outputs": [],
   "source": [
    "W21bar = V11T @ W21.T\n",
    "W32bar = U33.T @ W32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39c050-9c27-45f9-967b-d3c99c4daf58",
   "metadata": {},
   "source": [
    "Then in the new synaptic weight space, the fixed point $W^{32} W^{21} = C^{31}$ becomes\n",
    "\n",
    "$\\overline{W^{32}} \\overline{W^{21}}^T = S^{31}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "9444ca33-203e-4c09-9895-cee5c5a751ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03147067570905002"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((S31 - U33.T @ W32 @ W21 @ V11T) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9de99-d690-4a4d-ba32-8d658b5a97e4",
   "metadata": {},
   "source": [
    "Now, the gradient descent rules can be written in terms of $C^{31}$ without reference to the underlying dataset $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772884d-f0a6-40f9-8215-65b1b00eacf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
